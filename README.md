# whatsapp-ai-finetuning
ğŸ¤– WhatsApp Chatbot Fine-Tuning with TinyLlama
This project builds a personalized AI chatbot by fine-tuning the TinyLlama model on WhatsApp-style conversations or custom dialog pairs.

It uses LoRA (Low-Rank Adaptation) and 4-bit quantization to enable efficient training on low-resource GPUs like Google Colab (T4).

ğŸ“¦ Features
âœ… Fine-tunes TinyLlama on your WhatsApp/exported dialog data

âœ… Lightweight: 4-bit quantized + LoRA (RAM/GPU-friendly)

âœ… Builds a private, offline chatbot

âœ… Includes basic CLI chat interface

âœ… Easy to extend to Gradio/Telegram bots

ğŸ“ Project Structure
bash
Copy
Edit
.
â”œâ”€â”€ dialogs.txt              # Your instruction-response pairs (TSV)
â”œâ”€â”€ fine_tuned_model/        # Output: Trained model + tokenizer
â”œâ”€â”€ train_tinyllama.ipynb    # Main notebook for fine-tuning
â”œâ”€â”€ chat_interface.py        # CLI-based chat demo (optional)
â”œâ”€â”€ README.md
ğŸ› ï¸ Setup & Installation
Run this in a fresh Colab or your own environment:

bash
Copy
Edit
pip install whatstk einops bitsandbytes transformers peft accelerate datasets
ğŸ“¥ Step 1: Prepare Your Data
Option A: Dialog Pairs File (Recommended)
Format: dialogs.txt â€” tab-separated text file with:

tsv
Copy
Edit
Hi, how are you?	Iâ€™m doing well, thanks!
Whatâ€™s your name?	My name is VarunBot.
Option B: Raw WhatsApp Export (Optional)
python
Copy
Edit
from whatstk import WhatsAppChat

chat = WhatsAppChat.from_source("chat.txt")
df = chat.df[['date', 'name', 'text']]
Youâ€™ll need to clean, group, and structure this into (instruction, response) format.

ğŸ”„ Step 2: Convert to Alpaca Format
Transform dialog data into the format:

json
Copy
Edit
{
  "instruction": "User's message",
  "input": "",
  "output": "Bot's reply"
}
Use Python to save this as .jsonl.

ğŸ§  Step 3: Fine-Tune TinyLlama
In train_tinyllama.ipynb, you'll:

Load TinyLlama in 4-bit mode

Apply LoRA adapters

Format and tokenize dataset

Fine-tune with HuggingFace Trainer

Save model and tokenizer

ğŸ§ª Step 4: Inference (CLI Chatbot)
Use a simple CLI interface (or build a web UI later):

python
Copy
Edit
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("fine_tuned_model")
tokenizer = AutoTokenizer.from_pretrained("fine_tuned_model")

def chat(user_input):
    prompt = f"### Instruction:\n{user_input}\n\n### Response:\n"
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids
    output = model.generate(input_ids, max_new_tokens=100)
    print(tokenizer.decode(output[0], skip_special_tokens=True))

chat("Whatâ€™s your name?")
ğŸ§  How It Works
âœ… LoRA lets us fine-tune only a small subset of model weights.

âœ… Alpaca-style format helps the model understand instruction-following behavior.

âœ… The trained model mimics tone and responses from your data.

ğŸ’¡ Real-Life Use Cases
Personalized companion chatbot

Mental health / empathetic bots

Customer support assistant (chat-based)

Internal knowledge bot (train on team chats)

Educational tutor bots

Local language/dialect AI agents

ğŸš€ Next Steps
ğŸ”— Add Gradio UI for demo

ğŸ¤– Deploy as a Telegram or WhatsApp bot

â˜ï¸ Upload to Hugging Face Hub

ğŸŒ Train on multilingual or domain-specific datasets

ğŸ‘¤ Author & Credits
Built by Varun Haridas
Powered by HuggingFace Transformers, PEFT, and TinyLlama

