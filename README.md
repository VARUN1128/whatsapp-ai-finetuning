 ChatCraft: Fine-Tuned WhatsApp AI Assistant with TinyLlama + Gradio
Welcome to ChatCraft â€” a powerful pipeline to fine-tune lightweight language models on real conversational data (like WhatsApp chats) and deploy them with a sleek Gradio-powered web UI.

Whether you're building a personal AI companion, a customer support agent, or a domain-specific tutor, ChatCraft helps you train your own custom model using open-source tools and run it efficiently on minimal hardware.

ğŸš€ Demo: Web Chat Interface (Gradio)
With just one command, launch your AI assistant in the browser:

<!-- (replace with actual GIF if you have) -->

bash
Copy
Edit
python gradio_chat.py
ğŸŒŸ Project Highlights
âœ… Fine-tunes TinyLlama-1.1B-Chat
âœ… Trains on your WhatsApp chats or dialog datasets
âœ… Uses LoRA + 4-bit quantization â†’ train on Google Colab / laptop
âœ… Clean Gradio chat UI for instant testing
âœ… Easy to extend to Telegram / WhatsApp bot deployment
âœ… Fully offline and private

ğŸ“ Project Structure
bash
Copy
Edit
.
â”œâ”€â”€ dialogs.txt              # Chat data (instruction \t response)
â”œâ”€â”€ alpaca_data.jsonl        # Converted Alpaca-format dataset
â”œâ”€â”€ train_tinyllama.ipynb    # Fine-tuning notebook (TinyLlama + LoRA)
â”œâ”€â”€ gradio_chat.py           # Gradio-based web chatbot interface
â”œâ”€â”€ fine_tuned_model/        # Output directory after training
â”œâ”€â”€ README.md
ğŸ”§ Installation
Requirements
Python 3.8+

CUDA GPU (T4, A100, or Colab) recommended

Install Dependencies
bash
Copy
Edit
pip install whatstk einops gradio bitsandbytes transformers peft accelerate datasets
ğŸ“¥ Step 1: Prepare Your Data
Option A: Predefined Dialogs
Use dialogs.txt with this structure:

ts
Copy
Edit
Hi there!	Hey! How can I help you today?
What's your name?	I'm your AI assistant, trained just for you.
Option B: WhatsApp Export
python
Copy
Edit
from whatstk import WhatsAppChat
chat = WhatsAppChat.from_source("chat.txt")
df = chat.df[['date', 'name', 'text']]
Group and convert messages into dialog pairs using time gaps or names.

ğŸ”„ Step 2: Convert to Alpaca Format
Transforms into:

json
Copy
Edit
{"instruction": "Hi!", "input": "", "output": "Hello! How can I assist you?"}
Save as alpaca_data.jsonl.

ğŸ§  Step 3: Fine-Tune the Model
Open train_tinyllama.ipynb to:

Load TinyLlama in 4-bit mode

Apply LoRA adapters

Tokenize and train on your chat data

Save the fine-tuned model

Fine-tunes on Google Colab in under 2 hours!

ğŸ’¬ Step 4: Launch Gradio Chat UI
Run:

bash
Copy
Edit
python gradio_chat.py
This loads your fine-tuned model and serves a browser-based chatbot where anyone can talk to your custom-trained AI.

ğŸ’¼ Real-World Use Cases
ğŸ” Use Case	ğŸ’¡ Description
Personal AI Companion	Trained on your chat history or tone
Customer Support Chatbot	Trained on company FAQ / WhatsApp tickets
Mental Health Assistant	Mimics therapist-patient conversations
Educational Tutor	Learn from solved doubts and tutor chats
Regional Language Bot	Hinglish / Malayalam-trained assistant
Memory Bot	Remembers past messages / life events

ğŸŒ Next Steps
âœ… Add memory to Gradio chat (context-based chat)

ğŸŒ Deploy to Hugging Face Spaces or your server

ğŸ¤– Integrate with Telegram/WhatsApp bot

ğŸ§  Train on multilingual or multi-user data

ğŸ“¤ HuggingFace Integration (Optional)
Login and upload your fine-tuned model:

python
Copy
Edit
from huggingface_hub import login
login()
model.push_to_hub("your-model-name")
ğŸ‘¨â€ğŸ’» Built With
TinyLlama â€“ 1.1B instruction-tuned base model

LoRA â€“ Efficient fine-tuning method

Gradio â€“ Lightweight UI for demo and testing

HuggingFace Transformers â€“ Core ML framework

whatstk â€“ WhatsApp .txt parser

ğŸ‘¤ Author
Varun Haridas
Email: varun.haridas321@gmail.com
Made with â¤ï¸ for open-source AI

